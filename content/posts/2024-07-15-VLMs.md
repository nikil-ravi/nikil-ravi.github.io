---
title: "Vision Language Models"
date: 2024-07-15T10:14:00-05:00
draft: false
tags: ["vision-language-models", "AI", "computer-vision", "NLP"]
categories: ["research"]
description: "An overview of Vision Language Models and their applications"
ShowToc: true
TocOpen: false
math: true
---

# Introduction
Information theory assumes that all information can be represented using *bits*; a bit consists of either a 0 or a 1. We'd like ways to measure information, and also want techniques to represent and communicate it efficiently. In this post, we'll be going over fundamental, well-established notions in information theory.

# Measures of Information

Intuitively, a good measure of information would tell us only what we don't know- in other words, a measure of information should take into account how "surprising" the information is.

Suppose that $$U$$ is a random variable that may take on values in $$\mathbb{U} := \{1, 2, \cdots r\}$$. 

## Self-information

The smaller the probability of $$u \in \mathbb{U}$$, the more surprised we'd be if that value were actually realized by $$U$$. 

This is formalized by the notion of **self-information**, also known as the "surprise function" or simply as the "log loss".

$$s(u) := \log \frac{1}{p(u)}$$

## Entropy

The **entropy** of $$U$$ is defined as the expected surprise over all the possible realizations of $U$.

$$H(U) := \mathbb{E}_{U}[s(u)]$$

This can be expanded out to its most canonical form:

$$H(U) := \sum_{u = 1}^{r} p(u) \cdot log \frac{1}{p(u)}$$

Like other measures of quantities, entropy also has units- we refer to the units as **bits**/symbol for b = 2, **nats**/symbol for b = e, and **bans**/symbol for b = 10.

\TODO{Shannons, Hartleys}

We will also generalize $$H(U)$$ by defining $$H_q(U)$$; for a PMF $$q$$, which is possibly equal to $$p$$, $$H_q(U)$ := $\sum_{u = 1}^{r} p(u) \cdot log \frac{1}{q(u)}$$.  Intuitively, $$H_q$$ measures how surprised you are if you thought that $$U$$ comes from distribution $$q$$, but it was actually from $p$. 

When $$q = p$$, we these quantities are equal ($$H_q = H$$); in fact, we can even go further and say that $$H(U) \leq H_q(U)$$, with equality iff $$q = p$$ (the proof is tied to the notion of relative entropy, which immediately follows this section). All this is saying is that you're probably going to be more surprised if you got the distribution wrong.

## Relative Entropy

Now that we introduced a distribution $q$, it is useful to define a version of entropy that is *relative*, instead of absolute. This notion, **relative entropy**, can also be thought of as a measure of the divergence of the two distributions- in other words, how different is $$q$$ with respect to $$p$$?

$$
\begin{align}
D(p||q) &= H_q(U) - H(U)\\
        &= \mathbb{E}[\log \frac{1}{p(u)} - \log \frac{1}{q(u)}]\\
        &= \sum_{u=1}^{r} p(u) \cdot \left[ \log \frac{1}{p(u)} - \log \frac{1}{q(u)} \right]\\
        &= \sum_{u=1}^{r} p(u) \cdot \left( \log \frac{q(u)}{p(u)} \right)

\end{align}
$$

This quantity is also known as the Kullback-Leibler Divergence, or simply KL Divergence. Note that it is not symmetric; $$D(p ||q)$$ is not equal to $$D(q||p)$$ unless $$q = p$$. This is why we refer to it as a *divergence*, and not a distance in the sense of a metric.

The entropy of a random variable $$U$$ is upper-bounded by $$\log r$$ (proof idea: apply Jensen's inequality, since we have an expectation and a log).

## Joint Entropy

## Mutual Information
lol

# Typicality and the Asymptotic Equipartition Property

Now that we've defined some basic notions of information and its measurement, we can ask questions of the following kind: what are the characteristics of unsurprising information? What kinds of sequences occur typically?

An answer to this question emerges in the notion of *$$\epsilon$$-typical sequences*.







To add a table of contents to a post as a sidebar, simply add

```yml
toc:
  sidebar: left
```

to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change `left` to `right`.

### Example of Sub-Heading 1

Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.

### Example of another Sub-Heading 1

Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.

## Customizing Your Table of Contents

{:data-toc-text="Customizing"}

If you want to learn more about how to customize the table of contents of your sidebar, you can check the [bootstrap-toc](https://afeld.github.io/bootstrap-toc/) documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.

### Example of Sub-Heading 2

Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.

### Example of another Sub-Heading 2

Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.
