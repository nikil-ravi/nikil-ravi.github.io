[{"content":"Introduction Information theory assumes that all information can be represented using bits; a bit consists of either a 0 or a 1. We\u0026rsquo;d like ways to measure information, and also want techniques to represent and communicate it efficiently. In this post, we\u0026rsquo;ll be going over fundamental, well-established notions in information theory.\nMeasures of Information Intuitively, a good measure of information would tell us only what we don\u0026rsquo;t know- in other words, a measure of information should take into account how \u0026ldquo;surprising\u0026rdquo; the information is.\nSuppose that $$U$$ is a random variable that may take on values in $$\\mathbb{U} := {1, 2, \\cdots r}$$.\nSelf-information The smaller the probability of $$u \\in \\mathbb{U}$$, the more surprised we\u0026rsquo;d be if that value were actually realized by $$U$$.\nThis is formalized by the notion of self-information, also known as the \u0026ldquo;surprise function\u0026rdquo; or simply as the \u0026ldquo;log loss\u0026rdquo;.\n$$s(u) := \\log \\frac{1}{p(u)}$$\nEntropy The entropy of $$U$$ is defined as the expected surprise over all the possible realizations of $U$.\n$$H(U) := \\mathbb{E}_{U}[s(u)]$$\nThis can be expanded out to its most canonical form:\n$$H(U) := \\sum_{u = 1}^{r} p(u) \\cdot log \\frac{1}{p(u)}$$\nLike other measures of quantities, entropy also has units- we refer to the units as bits/symbol for b = 2, nats/symbol for b = e, and bans/symbol for b = 10.\n\\TODO{Shannons, Hartleys}\nWe will also generalize $$H(U)$$ by defining $$H_q(U)$$; for a PMF $$q$$, which is possibly equal to $$p$$, $$H_q(U)$ := $\\sum_{u = 1}^{r} p(u) \\cdot log \\frac{1}{q(u)}$$. Intuitively, $$H_q$$ measures how surprised you are if you thought that $$U$$ comes from distribution $$q$$, but it was actually from $p$.\nWhen $$q = p$$, we these quantities are equal ($$H_q = H$$); in fact, we can even go further and say that $$H(U) \\leq H_q(U)$$, with equality iff $$q = p$$ (the proof is tied to the notion of relative entropy, which immediately follows this section). All this is saying is that you\u0026rsquo;re probably going to be more surprised if you got the distribution wrong.\nRelative Entropy Now that we introduced a distribution $q$, it is useful to define a version of entropy that is relative, instead of absolute. This notion, relative entropy, can also be thought of as a measure of the divergence of the two distributions- in other words, how different is $$q$$ with respect to $$p$$?\n$$ \\begin{align} D(p||q) \u0026amp;= H_q(U) - H(U)\\ \u0026amp;= \\mathbb{E}[\\log \\frac{1}{p(u)} - \\log \\frac{1}{q(u)}]\\ \u0026amp;= \\sum_{u=1}^{r} p(u) \\cdot \\left[ \\log \\frac{1}{p(u)} - \\log \\frac{1}{q(u)} \\right]\\ \u0026amp;= \\sum_{u=1}^{r} p(u) \\cdot \\left( \\log \\frac{q(u)}{p(u)} \\right)\n\\end{align} $$\nThis quantity is also known as the Kullback-Leibler Divergence, or simply KL Divergence. Note that it is not symmetric; $$D(p ||q)$$ is not equal to $$D(q||p)$$ unless $$q = p$$. This is why we refer to it as a divergence, and not a distance in the sense of a metric.\nThe entropy of a random variable $$U$$ is upper-bounded by $$\\log r$$ (proof idea: apply Jensen\u0026rsquo;s inequality, since we have an expectation and a log).\nJoint Entropy Mutual Information lol\nTypicality and the Asymptotic Equipartition Property Now that we\u0026rsquo;ve defined some basic notions of information and its measurement, we can ask questions of the following kind: what are the characteristics of unsurprising information? What kinds of sequences occur typically?\nAn answer to this question emerges in the notion of $$\\epsilon$$-typical sequences.\nTo add a table of contents to a post as a sidebar, simply add\n1 2 toc: sidebar: left to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change left to right.\nExample of Sub-Heading 1 Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. Pinterest DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade cold-pressed meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.\nExample of another Sub-Heading 1 Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. Pinterest DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade cold-pressed meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.\nCustomizing Your Table of Contents {:data-toc-text=\u0026ldquo;Customizing\u0026rdquo;}\nIf you want to learn more about how to customize the table of contents of your sidebar, you can check the bootstrap-toc documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.\nExample of Sub-Heading 2 Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. Pinterest DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade cold-pressed meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.\nExample of another Sub-Heading 2 Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. Pinterest DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade cold-pressed meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.\n","permalink":"https://nikil-ravi.github.io/posts/2024-07-15-vlms/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eInformation theory assumes that all information can be represented using \u003cem\u003ebits\u003c/em\u003e; a bit consists of either a 0 or a 1. We\u0026rsquo;d like ways to measure information, and also want techniques to represent and communicate it efficiently. In this post, we\u0026rsquo;ll be going over fundamental, well-established notions in information theory.\u003c/p\u003e\n\u003ch1 id=\"measures-of-information\"\u003eMeasures of Information\u003c/h1\u003e\n\u003cp\u003eIntuitively, a good measure of information would tell us only what we don\u0026rsquo;t know- in other words, a measure of information should take into account how \u0026ldquo;surprising\u0026rdquo; the information is.\u003c/p\u003e","title":"Vision Language Models"},{"content":"Introduction Information theory assumes that all information can be represented using bits; a bit consists of either a 0 or a 1. We\u0026rsquo;d like ways to measure information, and also want techniques to represent and communicate it efficiently. In this post, we\u0026rsquo;ll be going over fundamental, well-established notions in information theory.\nMeasures of Information Intuitively, a good measure of information would tell us only what we don\u0026rsquo;t know- the idea is that if you only saw what you already knew, then the content wasn\u0026rsquo;t very informative. In other words, a measure of information should take into account how \u0026ldquo;surprising\u0026rdquo; the information is.\nIn order to formalize the surprising-ness of information, we will first need to define a prior distribution with respect to which we define this quantity.\nSuppose that $U$ is a random variable that may take on values in $\\mathbb{U} := {1, 2, \\cdots r}$. Note the subtle difference in notation between them- this is going to be used throughout.\nSelf-information The smaller the probability of $u \\in \\mathbb{U}$, the more surprised we\u0026rsquo;d be if that value were actually realized by $U$.\nThis is formalized by the notion of self-information, also known as the \u0026ldquo;surprise function\u0026rdquo; or simply as the \u0026ldquo;log loss\u0026rdquo;.\n$$s(u) := \\log \\frac{1}{p(u)}$$\nEntropy The entropy of $U$ is defined as the expected surprise over all the possible realizations of $U$.\n$$H(U) := \\mathbb{E}_{U}[s(u)]$$\nThis can be expanded out to its most canonical form:\n$$H(U) := \\sum_{u = 1}^{r} p(u) \\cdot log \\frac{1}{p(u)}$$\nLike other measures of quantities, entropy also has units- we refer to the units as bits/symbol for b = 2, nats/symbol for b = e, and bans/symbol for b = 10.\nJoint Entropy Conditional Entropy Mutual Information Consider the scenario in which there are two random variables $U$ and $V$. Suppose you have no prior knowledge about the values of these random variables. In this case the joint entropy $H(U, V)$ gives you the amount of information required to describe both of them.\nAlternatively, let\u0026rsquo;s say that you already know the value of $U$ but not that of $V$. In this case, the amount of additional information required to also describe $V$ is the conditional entropy $H(V | U)$.\nFrom the chain rule, we know that $H(U, V) = H(U) + H(V | U)$. Therefore, knowing $U$ reduces the amount of information required to describe $V$ by exactly $H(V) - H(V | U)$. This quantity is called the mutual information:\n$$I(U; V) := H(V) - H(V | U)$$\nMutual information is always non-negative; this aligns with our intuition that knowing one random variable should never increase the surprise (i.e., required information) of another.\n","permalink":"https://nikil-ravi.github.io/posts/2024-01-14-information-theory/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eInformation theory assumes that all information can be represented using \u003cem\u003ebits\u003c/em\u003e; a bit consists of either a 0 or a 1. We\u0026rsquo;d like ways to measure information, and also want techniques to represent and communicate it efficiently. In this post, we\u0026rsquo;ll be going over fundamental, well-established notions in information theory.\u003c/p\u003e\n\u003ch1 id=\"measures-of-information\"\u003eMeasures of Information\u003c/h1\u003e\n\u003cp\u003eIntuitively, a good measure of information would tell us only what we don\u0026rsquo;t know- the idea is that if you only saw what you already knew, then the content wasn\u0026rsquo;t very informative. In other words, a measure of information should take into account how \u0026ldquo;surprising\u0026rdquo; the information is.\u003c/p\u003e","title":"Information Theory"}]