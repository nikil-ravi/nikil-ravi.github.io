<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Vision Language Models | Nikil Ravi</title><meta name=keywords content="vision-language-models,AI,computer-vision,NLP"><meta name=description content="An overview of Vision Language Models and their applications"><meta name=author content="Nikil Ravi"><link rel=canonical href=http://localhost:1313/posts/2024-07-15-vlms/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/2024-07-15-vlms/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/posts/2024-07-15-vlms/"><meta property="og:site_name" content="Nikil Ravi"><meta property="og:title" content="Vision Language Models"><meta property="og:description" content="An overview of Vision Language Models and their applications"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-15T10:14:00-05:00"><meta property="article:modified_time" content="2024-07-15T10:14:00-05:00"><meta property="article:tag" content="Vision-Language-Models"><meta property="article:tag" content="AI"><meta property="article:tag" content="Computer-Vision"><meta property="article:tag" content="NLP"><meta name=twitter:card content="summary"><meta name=twitter:title content="Vision Language Models"><meta name=twitter:description content="An overview of Vision Language Models and their applications"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Vision Language Models","item":"http://localhost:1313/posts/2024-07-15-vlms/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Vision Language Models","name":"Vision Language Models","description":"An overview of Vision Language Models and their applications","keywords":["vision-language-models","AI","computer-vision","NLP"],"articleBody":"Introduction Information theory assumes that all information can be represented using bits; a bit consists of either a 0 or a 1. We’d like ways to measure information, and also want techniques to represent and communicate it efficiently. In this post, we’ll be going over fundamental, well-established notions in information theory.\nMeasures of Information Intuitively, a good measure of information would tell us only what we don’t know- in other words, a measure of information should take into account how “surprising” the information is.\nSuppose that $$U$$ is a random variable that may take on values in $$\\mathbb{U} := {1, 2, \\cdots r}$$.\nSelf-information The smaller the probability of $$u \\in \\mathbb{U}$$, the more surprised we’d be if that value were actually realized by $$U$$.\nThis is formalized by the notion of self-information, also known as the “surprise function” or simply as the “log loss”.\n$$s(u) := \\log \\frac{1}{p(u)}$$\nEntropy The entropy of $$U$$ is defined as the expected surprise over all the possible realizations of $U$.\n$$H(U) := \\mathbb{E}_{U}[s(u)]$$\nThis can be expanded out to its most canonical form:\n$$H(U) := \\sum_{u = 1}^{r} p(u) \\cdot log \\frac{1}{p(u)}$$\nLike other measures of quantities, entropy also has units- we refer to the units as bits/symbol for b = 2, nats/symbol for b = e, and bans/symbol for b = 10.\n\\TODO{Shannons, Hartleys}\nWe will also generalize $$H(U)$$ by defining $$H_q(U)$$; for a PMF $$q$$, which is possibly equal to $$p$$, $$H_q(U)$ := $\\sum_{u = 1}^{r} p(u) \\cdot log \\frac{1}{q(u)}$$. Intuitively, $$H_q$$ measures how surprised you are if you thought that $$U$$ comes from distribution $$q$$, but it was actually from $p$.\nWhen $$q = p$$, we these quantities are equal ($$H_q = H$$); in fact, we can even go further and say that $$H(U) \\leq H_q(U)$$, with equality iff $$q = p$$ (the proof is tied to the notion of relative entropy, which immediately follows this section). All this is saying is that you’re probably going to be more surprised if you got the distribution wrong.\nRelative Entropy Now that we introduced a distribution $q$, it is useful to define a version of entropy that is relative, instead of absolute. This notion, relative entropy, can also be thought of as a measure of the divergence of the two distributions- in other words, how different is $$q$$ with respect to $$p$$?\n$$ \\begin{align} D(p||q) \u0026= H_q(U) - H(U)\\ \u0026= \\mathbb{E}[\\log \\frac{1}{p(u)} - \\log \\frac{1}{q(u)}]\\ \u0026= \\sum_{u=1}^{r} p(u) \\cdot \\left[ \\log \\frac{1}{p(u)} - \\log \\frac{1}{q(u)} \\right]\\ \u0026= \\sum_{u=1}^{r} p(u) \\cdot \\left( \\log \\frac{q(u)}{p(u)} \\right)\n\\end{align} $$\nThis quantity is also known as the Kullback-Leibler Divergence, or simply KL Divergence. Note that it is not symmetric; $$D(p ||q)$$ is not equal to $$D(q||p)$$ unless $$q = p$$. This is why we refer to it as a divergence, and not a distance in the sense of a metric.\nThe entropy of a random variable $$U$$ is upper-bounded by $$\\log r$$ (proof idea: apply Jensen’s inequality, since we have an expectation and a log).\nJoint Entropy Mutual Information lol\nTypicality and the Asymptotic Equipartition Property Now that we’ve defined some basic notions of information and its measurement, we can ask questions of the following kind: what are the characteristics of unsurprising information? What kinds of sequences occur typically?\nAn answer to this question emerges in the notion of $$\\epsilon$$-typical sequences.\nTo add a table of contents to a post as a sidebar, simply add\n1 2 toc: sidebar: left to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change left to right.\nExample of Sub-Heading 1 Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. Pinterest DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade cold-pressed meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.\nExample of another Sub-Heading 1 Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. Pinterest DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade cold-pressed meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.\nCustomizing Your Table of Contents {:data-toc-text=“Customizing”}\nIf you want to learn more about how to customize the table of contents of your sidebar, you can check the bootstrap-toc documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.\nExample of Sub-Heading 2 Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. Pinterest DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade cold-pressed meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.\nExample of another Sub-Heading 2 Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. Pinterest DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade cold-pressed meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.\n","wordCount":"1027","inLanguage":"en","datePublished":"2024-07-15T10:14:00-05:00","dateModified":"2024-07-15T10:14:00-05:00","author":{"@type":"Person","name":"Nikil Ravi"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/2024-07-15-vlms/"},"publisher":{"@type":"Organization","name":"Nikil Ravi","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Nikil Ravi (Alt + H)">Nikil Ravi</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/work/ title=Work><span>Work</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Vision Language Models</h1><div class=post-description>An overview of Vision Language Models and their applications</div><div class=post-meta><span title='2024-07-15 10:14:00 -0500 -0500'>July 15, 2024</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;1027 words&nbsp;·&nbsp;Nikil Ravi</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#self-information>Self-information</a></li><li><a href=#entropy>Entropy</a></li><li><a href=#relative-entropy>Relative Entropy</a></li><li><a href=#joint-entropy>Joint Entropy</a></li><li><a href=#mutual-information>Mutual Information</a></li></ul><ul><li><ul><li><a href=#example-of-sub-heading-1>Example of Sub-Heading 1</a></li><li><a href=#example-of-another-sub-heading-1>Example of another Sub-Heading 1</a></li></ul></li><li><a href=#customizing-your-table-of-contents>Customizing Your Table of Contents</a><ul><li><a href=#example-of-sub-heading-2>Example of Sub-Heading 2</a></li><li><a href=#example-of-another-sub-heading-2>Example of another Sub-Heading 2</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Information theory assumes that all information can be represented using <em>bits</em>; a bit consists of either a 0 or a 1. We&rsquo;d like ways to measure information, and also want techniques to represent and communicate it efficiently. In this post, we&rsquo;ll be going over fundamental, well-established notions in information theory.</p><h1 id=measures-of-information>Measures of Information<a hidden class=anchor aria-hidden=true href=#measures-of-information>#</a></h1><p>Intuitively, a good measure of information would tell us only what we don&rsquo;t know- in other words, a measure of information should take into account how &ldquo;surprising&rdquo; the information is.</p><p>Suppose that $$U$$ is a random variable that may take on values in $$\mathbb{U} := {1, 2, \cdots r}$$.</p><h2 id=self-information>Self-information<a hidden class=anchor aria-hidden=true href=#self-information>#</a></h2><p>The smaller the probability of $$u \in \mathbb{U}$$, the more surprised we&rsquo;d be if that value were actually realized by $$U$$.</p><p>This is formalized by the notion of <strong>self-information</strong>, also known as the &ldquo;surprise function&rdquo; or simply as the &ldquo;log loss&rdquo;.</p><p>$$s(u) := \log \frac{1}{p(u)}$$</p><h2 id=entropy>Entropy<a hidden class=anchor aria-hidden=true href=#entropy>#</a></h2><p>The <strong>entropy</strong> of $$U$$ is defined as the expected surprise over all the possible realizations of $U$.</p><p>$$H(U) := \mathbb{E}_{U}[s(u)]$$</p><p>This can be expanded out to its most canonical form:</p><p>$$H(U) := \sum_{u = 1}^{r} p(u) \cdot log \frac{1}{p(u)}$$</p><p>Like other measures of quantities, entropy also has units- we refer to the units as <strong>bits</strong>/symbol for b = 2, <strong>nats</strong>/symbol for b = e, and <strong>bans</strong>/symbol for b = 10.</p><p>\TODO{Shannons, Hartleys}</p><p>We will also generalize $$H(U)$$ by defining $$H_q(U)$$; for a PMF $$q$$, which is possibly equal to $$p$$, $$H_q(U)$ := $\sum_{u = 1}^{r} p(u) \cdot log \frac{1}{q(u)}$$. Intuitively, $$H_q$$ measures how surprised you are if you thought that $$U$$ comes from distribution $$q$$, but it was actually from $p$.</p><p>When $$q = p$$, we these quantities are equal ($$H_q = H$$); in fact, we can even go further and say that $$H(U) \leq H_q(U)$$, with equality iff $$q = p$$ (the proof is tied to the notion of relative entropy, which immediately follows this section). All this is saying is that you&rsquo;re probably going to be more surprised if you got the distribution wrong.</p><h2 id=relative-entropy>Relative Entropy<a hidden class=anchor aria-hidden=true href=#relative-entropy>#</a></h2><p>Now that we introduced a distribution $q$, it is useful to define a version of entropy that is <em>relative</em>, instead of absolute. This notion, <strong>relative entropy</strong>, can also be thought of as a measure of the divergence of the two distributions- in other words, how different is $$q$$ with respect to $$p$$?</p><p>$$
\begin{align}
D(p||q) &= H_q(U) - H(U)\
&= \mathbb{E}[\log \frac{1}{p(u)} - \log \frac{1}{q(u)}]\
&= \sum_{u=1}^{r} p(u) \cdot \left[ \log \frac{1}{p(u)} - \log \frac{1}{q(u)} \right]\
&= \sum_{u=1}^{r} p(u) \cdot \left( \log \frac{q(u)}{p(u)} \right)</p><p>\end{align}
$$</p><p>This quantity is also known as the Kullback-Leibler Divergence, or simply KL Divergence. Note that it is not symmetric; $$D(p ||q)$$ is not equal to $$D(q||p)$$ unless $$q = p$$. This is why we refer to it as a <em>divergence</em>, and not a distance in the sense of a metric.</p><p>The entropy of a random variable $$U$$ is upper-bounded by $$\log r$$ (proof idea: apply Jensen&rsquo;s inequality, since we have an expectation and a log).</p><h2 id=joint-entropy>Joint Entropy<a hidden class=anchor aria-hidden=true href=#joint-entropy>#</a></h2><h2 id=mutual-information>Mutual Information<a hidden class=anchor aria-hidden=true href=#mutual-information>#</a></h2><p>lol</p><h1 id=typicality-and-the-asymptotic-equipartition-property>Typicality and the Asymptotic Equipartition Property<a hidden class=anchor aria-hidden=true href=#typicality-and-the-asymptotic-equipartition-property>#</a></h1><p>Now that we&rsquo;ve defined some basic notions of information and its measurement, we can ask questions of the following kind: what are the characteristics of unsurprising information? What kinds of sequences occur typically?</p><p>An answer to this question emerges in the notion of <em>$$\epsilon$$-typical sequences</em>.</p><p>To add a table of contents to a post as a sidebar, simply add</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1>1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-yml data-lang=yml><span class=line><span class=cl><span class=nt>toc</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>sidebar</span><span class=p>:</span><span class=w> </span><span class=l>left</span><span class=w>
</span></span></span></code></pre></td></tr></table></div></div><p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change <code>left</code> to <code>right</code>.</p><h3 id=example-of-sub-heading-1>Example of Sub-Heading 1<a hidden class=anchor aria-hidden=true href=#example-of-sub-heading-1>#</a></h3><p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href=https://www.pinterest.com>Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href=https://en.wikipedia.org/wiki/Cold-pressed_juice>cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p><h3 id=example-of-another-sub-heading-1>Example of another Sub-Heading 1<a hidden class=anchor aria-hidden=true href=#example-of-another-sub-heading-1>#</a></h3><p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href=https://www.pinterest.com>Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href=https://en.wikipedia.org/wiki/Cold-pressed_juice>cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p><h2 id=customizing-your-table-of-contents>Customizing Your Table of Contents<a hidden class=anchor aria-hidden=true href=#customizing-your-table-of-contents>#</a></h2><p>{:data-toc-text=&ldquo;Customizing&rdquo;}</p><p>If you want to learn more about how to customize the table of contents of your sidebar, you can check the <a href=https://afeld.github.io/bootstrap-toc/>bootstrap-toc</a> documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.</p><h3 id=example-of-sub-heading-2>Example of Sub-Heading 2<a hidden class=anchor aria-hidden=true href=#example-of-sub-heading-2>#</a></h3><p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href=https://www.pinterest.com>Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href=https://en.wikipedia.org/wiki/Cold-pressed_juice>cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p><h3 id=example-of-another-sub-heading-2>Example of another Sub-Heading 2<a hidden class=anchor aria-hidden=true href=#example-of-another-sub-heading-2>#</a></h3><p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href=https://www.pinterest.com>Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href=https://en.wikipedia.org/wiki/Cold-pressed_juice>cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/vision-language-models/>Vision-Language-Models</a></li><li><a href=http://localhost:1313/tags/ai/>AI</a></li><li><a href=http://localhost:1313/tags/computer-vision/>Computer-Vision</a></li><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/2024-01-14-information-theory/><span class=title>Next »</span><br><span>Information Theory</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Nikil Ravi</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>